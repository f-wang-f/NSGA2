clc; clear; close all;
warning off

%% 0. 检查 MATLAB 版本
matlabVersion = version('-release');
if str2double(matlabVersion(1:4)) < 2019
    error('需要 MATLAB R2019b 或更高版本。当前版本：%s', matlabVersion);
end
if ~license('test', 'neural_network_toolbox')
    error('未检测到 Deep Learning Toolbox。请确保已安装该工具箱。');
end
fprintf('MATLAB 版本：%s\n', matlabVersion);

%% 1. 数据读取与预处理
fprintf('正在读取风速数据...\n');
filename = 'winddata.xlsx'; % 数据文件名

try
    data = readtable(filename, 'VariableNamingRule', 'preserve');
    possibleColumnNames = {'Wind Speed (m/s)', 'WindSpeed(m/s)', 'Wind Speed', 'Speed'};
    found = false;
    for i = 1:length(possibleColumnNames)
        if ismember(possibleColumnNames{i}, data.Properties.VariableNames)
            speedData = data{:, possibleColumnNames{i}};
            windSpeedColumnName = possibleColumnNames{i};
            found = true;
            break;
        end
    end
    if ~found
        fprintf('可用列名：\n');
        fprintf('  %s\n', data.Properties.VariableNames{:});
        error('未找到风速数据列。');
    end
catch
    error('无法读取 Excel 文件。请检查文件路径、名称或格式。');
end

% 数据清洗：移除 NaN 和 Inf
speedData = speedData(~isnan(speedData) & ~isinf(speedData));
if length(speedData) < 100
    error('数据不足（少于100个样本），请提供更多数据。');
end

% 数据归一化 - Min-Max 归一化
minSpeed = min(speedData);
maxSpeed = max(speedData);
speedDataNorm = (speedData - minSpeed) / (maxSpeed - minSpeed);

% 创建时间序列数据集
sequenceLength = 50;
numFeatures = 1;

% 创建细胞数组，每个元素是一个序列样本 [1xsequenceLength]
X = cell(length(speedDataNorm) - sequenceLength, 1);
Y = zeros(length(speedDataNorm) - sequenceLength, 1);

for i = 1:(length(speedDataNorm) - sequenceLength)
    X{i} = speedDataNorm(i:i+sequenceLength-1)';  % 创建行向量 [1x50]
    Y(i) = speedDataNorm(i+sequenceLength);      % 目标值
end

% 拆分为训练集、验证集、测试集
trainRatio = 0.7;  % 训练集比例为 70%
valRatio = 0.15;   % 验证集比例为 15%
testRatio = 0.15;  % 测试集比例为 15%

numSamples = length(X);
numTrain = floor(trainRatio * numSamples);
numVal = floor(valRatio * numSamples);
numTest = numSamples - numTrain - numVal;

XTrain = X(1:numTrain);       % 训练集
YTrain = Y(1:numTrain);       % 训练目标
XVal = X(numTrain+1:numTrain+numVal);     % 验证集
YVal = Y(numTrain+1:numTrain+numVal);     % 验证目标
XTest = X(numTrain+numVal+1:end);         % 测试集
YTest = Y(numTrain+numVal+1:end);         % 测试目标

% 定义验证集和测试集真实值（反归一化）
YVal_actual = YVal * (maxSpeed - minSpeed) + minSpeed;
YTest_actual = YTest * (maxSpeed - minSpeed) + minSpeed;

fprintf('数据预处理完成。训练样本数：%d，验证样本数：%d，测试样本数：%d\n', ...
    numTrain, numVal, numTest);

%% 2. 定义超参数优化目标函数
% 在原来10个变量基础上，新增以下超参数：
% 11 optimizerType (1=adam,2=sgdm,3=rmsprop)
% 12 numConvLayers (1 or 2)
% 13 convDropout (0.0 - 0.5)
% 14 activationFunction (1=ReLU,2=LeakyReLU,3=Tanh,4=Sigmoid)
% 其它原有参数保持不变

%% 3. 手动实现 NSGA-II 超参数优化（保留原来框架，扩展变量）
fprintf('开始 NSGA-II 超参数优化（扩展超参数：optimizer、convLayers、convDropout、activationFunction）...\n');

% 优化定义范围（14变量）优化% 变量顺序：
% 1 BatchSize, 2 LearnRate, 3 PoolType, 4 NumFilters1, 5 FilterSize1,
% 6 LSTMUnits1, 7 LSTMUnits2, 8 RegType, 9 DropoutProb1, 10 DropoutProb2,
% 11 OptimizerType, 12 NumConvLayers, 13 ConvDropout，14 ActivationFunction
lb = [32, 1e-6, 1, 16, 2, 32, 32, 1, 0.2, 0.2, 1, 1, 0.1, 1];  % 优化后的下界
ub = [128, 1e-3, 2, 256, 5, 128, 128, 3, 0.4, 0.4, 3, 2, 0.3, 4];  % 优化后的上界
intCon = [1,3,4,5,6,7,8,11,12,14];  % 整数参数索引（保持不变）

% NSGA-II 参数（沿用原来设置）
populationSize = 2;  % 种群大小（调试时可缩小）
maxGenerations = 20;  % 进化代数
crossoverFraction = 0.8;
mutationRate = 0.1;

% 初始化种群
numVars = length(lb);
population = initializePopulation(populationSize, lb, ub, intCon);

% 记录历史（修改为结构化存储Pareto前沿）
bestParamsHistory = zeros(maxGenerations, numVars);
bestPerformanceHistory = zeros(maxGenerations, 1);
bestComplexityHistory = zeros(maxGenerations, 1);
allParetoFronts = cell(maxGenerations, 1);  % 存储每代的Pareto结构体

% 新增：存储特定代数的详细数据
specificGens = [1, 5, 10, 20, 30, 50]; % 要特别记录的代数
specificGenData = containers.Map('KeyType', 'int32', 'ValueType', 'any');

% 绘图准备
figure('Position', [100, 100, 800, 600]);
hold on;
xlabel('模型复杂度（越小越好）');
ylabel('预测误差 (RMSE, m/s)（越小越好）');
title('每一代的 Pareto 前沿（扩展超参）');
grid on;

% 进化主循环
for generation = 1:maxGenerations
    fprintf('第 %d 代进化中...\n', generation);
    
    % 评估种群
    [performance, complexity, rmse, mae, mape, r] = evaluatePopulation(population, XTrain, YTrain, XVal, YVal, minSpeed, maxSpeed, sequenceLength, numFeatures);
    
    % 记录当前代最佳解
    [minPerformance, minIdx] = min(performance);
    bestParamsHistory(generation, :) = population(minIdx, :);
    bestPerformanceHistory(generation) = minPerformance;
    bestComplexityHistory(generation) = complexity(minIdx);
    
    % 快速非支配排序
    [fronts, rank] = fastNonDominatedSort(performance, complexity);
    
    % 拥挤距离计算
    distance = improvedCrowdingDistance(performance, complexity, fronts);
    
    % 记录并绘制当前代的 Pareto 前沿（使用修正的结构化函数）
    paretoStruct = findParetoFront(population, performance, complexity);
    allParetoFronts{generation} = paretoStruct;
    
    % 新增：存储特定代数的详细数据
    if ismember(generation, specificGens)
        genData = struct();
        genData.population = population;
        genData.performance = performance;
        genData.complexity = complexity;
        genData.paretoFront = paretoStruct;  % 存储结构化Pareto前沿
        genData.rmse = rmse;
        genData.mae = mae;
        genData.mape = mape;
        genData.r = r;
        specificGenData(generation) = genData;
    end
    
    % 绘制当前代Pareto前沿
    if paretoStruct.numSolutions > 0
        perf_vals = paretoStruct.performance;
        complexity_vals = paretoStruct.complexity;
        
        uniquePoints = unique([complexity_vals, perf_vals], 'rows');
        perturbedFront = [];
        for i = 1:size(uniquePoints, 1)
            matchingRows = (complexity_vals == uniquePoints(i,1)) & (perf_vals == uniquePoints(i,2));
            numDuplicates = sum(matchingRows);
            if numDuplicates > 1
                jitter = 1e-4 * randn(numDuplicates, 2);
                perturbedPoints = repmat(uniquePoints(i, :), numDuplicates, 1) + jitter;
            else
                perturbedPoints = uniquePoints(i, :);
            end
            perturbedFront = [perturbedFront; perturbedPoints];
        end
        scatter(perturbedFront(:, 1), perturbedFront(:, 2), 36, 'filled');
        drawnow;
    end
    
    fprintf('第 %d 代 Pareto 解的数量: %d\n', generation, paretoStruct.numSolutions);
    
    % 选择
    matingPool = tournamentSelection(population, rank, distance, crossoverFraction, populationSize);
    
    % 交叉和变异
    offspring = crossoverAndMutation(matingPool, lb, ub, intCon, mutationRate);
    
    % 评估子代
    [offspringPerformance, offspringComplexity, ~, ~, ~, ~] = evaluatePopulation(offspring, XTrain, YTrain, XVal, YVal, minSpeed, maxSpeed, sequenceLength, numFeatures);
    
    % 合并种群
    combinedPopulation = [population; offspring];
    combinedPerformance = [performance; offspringPerformance];
    combinedComplexity = [complexity; offspringComplexity];
    
    % 对合并种群进行快速非支配排序
    [combinedFronts, combinedRank] = fastNonDominatedSort(combinedPerformance, combinedComplexity);
    combinedDistance = improvedCrowdingDistance(combinedPerformance, combinedComplexity, combinedFronts);
    
    % 环境选择
    [newPopulation, newPerformance, newComplexity] = environmentalSelection(...
        combinedPopulation, combinedPerformance, combinedComplexity, combinedRank, combinedDistance, populationSize);
    
    % 更新种群
    population = newPopulation;
    performance = newPerformance;
    complexity = newComplexity;
end

legend('show');
hold off;
fprintf('NSGA-II 优化完成。\n');

%% 4. 保存优化结果
finalParetoStruct = allParetoFronts{end};
save('nsga2_optimization_results_extended.mat', 'finalParetoStruct', 'allParetoFronts', 'bestParamsHistory', ...
     'bestPerformanceHistory', 'bestComplexityHistory', 'specificGenData');
fprintf('优化结果已保存至 nsga2_optimization_results_extended.mat\n');

%% 5. 最终模型训练（选取最后Pareto最优一代的折中最优解）
finalParetoStruct = allParetoFronts{end};
if finalParetoStruct.numSolutions > 0
    perf_vals = finalParetoStruct.performance;
    complexity_vals = finalParetoStruct.complexity;
    
    % 计算归一化值
    normalized_perf = (perf_vals - min(perf_vals)) / (max(perf_vals) - min(perf_vals) + eps);
    normalized_comp = (complexity_vals - min(complexity_vals)) / (max(complexity_vals) - min(complexity_vals) + eps);
    
    % 计算折中分数，例如欧几里得距离到理想点 (0,0)
    trade_off_scores = sqrt(normalized_perf.^2 + normalized_comp.^2);
    
    % 选择分数最小的作为折中最优（最接近理想点）
    [~, tradeOffIdx] = min(trade_off_scores);
    bestParams = finalParetoStruct.params(tradeOffIdx, :);
    fprintf('选取Pareto前沿折中最优解（trade-off index: %d）。\n', tradeOffIdx);
else
    error('最终Pareto前沿无有效解，无法选取模型。');
end

% 解析最优超参数
batchSize = round(bestParams(1));
learnRate = bestParams(2);
poolType = round(bestParams(3));
numFilters1 = round(bestParams(4));
filterSize1 = round(bestParams(5));
lstmUnits1 = round(bestParams(6));
lstmUnits2 = round(bestParams(7));
regType = round(bestParams(8));
dropoutProb1 = bestParams(9);
dropoutProb2 = bestParams(10);
optimizerType = round(bestParams(11));
numConvLayers = round(bestParams(12));
convDropout = bestParams(13);
activationFunction = round(bestParams(14)); % 激活函数类型

% 构建最终网络（与 evaluateModel 中逻辑一致）
minSeqLength = min(cellfun(@length, XTrain));

% conv padding 固定为 'same'（兼容 1D conv）
convPadding = 'same';

layers = [
    sequenceInputLayer(numFeatures, 'MinLength', minSeqLength)
    convolution1dLayer(filterSize1, numFilters1, 'Padding', convPadding)
    batchNormalizationLayer
];

% 根据激活函数类型添加相应层
switch activationFunction
    case 1 % ReLU
        layers = [layers; reluLayer];
    case 2 % LeakyReLU
        layers = [layers; leakyReluLayer(0.01)];
    case 3 % Tanh
        layers = [layers; tanhLayer];
    case 4 % Sigmoid
        layers = [layers; sigmoidLayer];
end

if convDropout > 0
    layers = [layers; dropoutLayer(convDropout)];
end

if numConvLayers == 2
    numFilters2 = max(8, round(numFilters1 / 2));
    layers = [layers;
        convolution1dLayer(filterSize1, numFilters2, 'Padding', convPadding)
        batchNormalizationLayer];
    
    % 第二个卷积层的激活函数
    switch activationFunction
        case 1 % ReLU
            layers = [layers; reluLayer];
        case 2 % LeakyReLU
            layers = [layers; leakyReluLayer(0.01)];
        case 3 % Tanh
            layers = [layers; tanhLayer];
        case 4 % Sigmoid
            layers = [layers; sigmoidLayer];
    end
    
    if convDropout > 0
        layers = [layers; dropoutLayer(convDropout)];
    end
    rnInputSize = numFilters2;
else
    rnInputSize = numFilters1;
end

% 池化（使用原 getPoolingLayer）
layers = [layers; getPoolingLayer(poolType, sequenceLength, filterSize1)];

% RNN：保留原 BiLSTM 结构（与原脚本一致）
layers = [layers;
    bilstmLayer(lstmUnits1, 'OutputMode', 'sequence')
    dropoutLayer(dropoutProb1)
    bilstmLayer(lstmUnits2, 'OutputMode', 'last')
    dropoutLayer(dropoutProb2)
    fullyConnectedLayer(1)
    regressionLayer];

% 正则化映射（近似）
if regType == 1 || regType == 3
    l2reg = 1e-4;
else
    l2reg = 0;
end

% 优化器映射
if optimizerType == 1
    solver = 'adam';
elseif optimizerType == 2
    solver = 'sgdm';
else
    solver = 'rmsprop';
end

% 训练最终模型
if strcmp(solver, 'sgdm')
    options = trainingOptions(solver, ...
        'MaxEpochs', 80, ...
        'MiniBatchSize', batchSize, ...
        'InitialLearnRate', learnRate, ...
        'L2Regularization', l2reg, ...
        'Momentum', 0.9, ...
        'GradientThreshold', 1, ...
        'ValidationData', {XVal, YVal}, ...
        'ValidationFrequency', 10, ...
        'ValidationPatience', 5, ...
        'Plots', 'training-progress', ...
        'Verbose', true);
else
    options = trainingOptions(solver, ...
        'MaxEpochs', 80, ...
        'MiniBatchSize', batchSize, ...
        'InitialLearnRate', learnRate, ...
        'L2Regularization', l2reg, ...
        'GradientThreshold', 1, ...
        'ValidationData', {XVal, YVal}, ...
        'ValidationFrequency', 10, ...
        'ValidationPatience', 5, ...
        'Plots', 'training-progress', ...
        'Verbose', true);
end

try
    net_opt = trainNetwork(XTrain, YTrain, layers, options);

    % 验证集预测与评估
    YValPred_opt = predict(net_opt, XVal);
    YValPred_opt_actual = YValPred_opt * (maxSpeed - minSpeed) + minSpeed;

    mae_val_opt = mean(abs(YVal_actual - YValPred_opt_actual));
    rmse_val_opt = sqrt(mean((YVal_actual - YValPred_opt_actual).^2));
    mape_val_opt = mean(abs((YVal_actual - YValPred_opt_actual)./YVal_actual)) * 100;
    corrMatrix_val_opt = corrcoef(YVal_actual, YValPred_opt_actual);
    if ~isempty(corrMatrix_val_opt) && size(corrMatrix_val_opt,1) >= 2
        r_val_opt = corrMatrix_val_opt(1,2);
    else
        r_val_opt = 0;
    end
    fprintf('优化模型验证集性能: MAE = %.4f (m/s), RMSE = %.4f (m/s), MAPE = %.2f%%, R = %.4f\n', mae_val_opt, rmse_val_opt, mape_val_opt, r_val_opt);

    % 测试集预测与评估
    YTestPred_opt = predict(net_opt, XTest);
    YTestPred_opt_actual = YTestPred_opt * (maxSpeed - minSpeed) + minSpeed;

    mae_test_opt = mean(abs(YTest_actual - YTestPred_opt_actual));
    rmse_test_opt = sqrt(mean((YTest_actual - YTestPred_opt_actual).^2));
    mape_test_opt = mean(abs((YTest_actual - YTestPred_opt_actual)./YTest_actual)) * 100;
    corrMatrix_test_opt = corrcoef(YTest_actual, YTestPred_opt_actual);
    if ~isempty(corrMatrix_test_opt) && size(corrMatrix_test_opt,1) >= 2
        r_test_opt = corrMatrix_test_opt(1,2);
    else
        r_test_opt = 0;
    end
    fprintf('优化模型测试集性能: MAE = %.4f (m/s), RMSE = %.4f (m/s), MAPE = %.2f%%, R = %.4f\n', mae_test_opt, rmse_test_opt, mape_test_opt, r_test_opt);

catch e
    warning('最终模型训练失败：%s', e.message);
end

%% ===== 基准模型对比（包含修正的Transformer） =====
fprintf('开始基准模型对比（包含修正的Transformer模型）...\n');

% 基准模型1: 纯LSTM模型
layers_lstm = [
    sequenceInputLayer(numFeatures, 'MinLength', minSeqLength)
    lstmLayer(lstmUnits1, 'OutputMode', 'sequence')
    dropoutLayer(dropoutProb1)
    lstmLayer(lstmUnits2, 'OutputMode', 'last')
    dropoutLayer(dropoutProb2)
    fullyConnectedLayer(1)
    regressionLayer];

options_lstm = trainingOptions('adam', ...
    'MaxEpochs', 80, ...
    'MiniBatchSize', batchSize, ...
    'InitialLearnRate', learnRate, ...
    'L2Regularization', l2reg, ...
    'GradientThreshold', 1, ...
    'ValidationData', {XVal, YVal}, ...
    'ValidationFrequency', 10, ...
    'ValidationPatience', 5, ...
    'Plots', 'none', ...
    'Verbose', true);

try
    net_lstm = trainNetwork(XTrain, YTrain, layers_lstm, options_lstm);
    YTestPred_lstm = predict(net_lstm, XTest);
    YTestPred_lstm_actual = YTestPred_lstm * (maxSpeed - minSpeed) + minSpeed;
    mae_test_lstm = mean(abs(YTest_actual - YTestPred_lstm_actual));
    rmse_test_lstm = sqrt(mean((YTest_actual - YTestPred_lstm_actual).^2));
    mape_test_lstm = mean(abs((YTest_actual - YTestPred_lstm_actual)./YTest_actual)) * 100;
    corrMatrix_test_lstm = corrcoef(YTest_actual, YTestPred_lstm_actual);
    r_test_lstm = corrMatrix_test_lstm(1,2);
    fprintf('LSTM模型测试集性能: MAE = %.4f, RMSE = %.4f, MAPE = %.2f%%, R = %.4f\n', mae_test_lstm, rmse_test_lstm, mape_test_lstm, r_test_lstm);
catch e
    warning('LSTM模型训练失败：%s', e.message);
    mae_test_lstm = NaN; rmse_test_lstm = NaN; mape_test_lstm = NaN; r_test_lstm = NaN;
    YTestPred_lstm_actual = NaN * ones(size(YTest_actual));
end

% 基准模型2: GRU模型
layers_gru = [
    sequenceInputLayer(numFeatures, 'MinLength', minSeqLength)
    gruLayer(lstmUnits1, 'OutputMode', 'sequence')
    dropoutLayer(dropoutProb1)
    gruLayer(lstmUnits2, 'OutputMode', 'last')
    dropoutLayer(dropoutProb2)
    fullyConnectedLayer(1)
    regressionLayer];

options_gru = trainingOptions('adam', ...
    'MaxEpochs', 80, ...
    'MiniBatchSize', batchSize, ...
    'InitialLearnRate', learnRate, ...
    'L2Regularization', l2reg, ...
    'GradientThreshold', 1, ...
    'ValidationData', {XVal, YVal}, ...
    'ValidationFrequency', 10, ...
    'ValidationPatience', 5, ...
    'Plots', 'none', ...
    'Verbose', true);

try
    net_gru = trainNetwork(XTrain, YTrain, layers_gru, options_gru);
    YTestPred_gru = predict(net_gru, XTest);
    YTestPred_gru_actual = YTestPred_gru * (maxSpeed - minSpeed) + minSpeed;
    mae_test_gru = mean(abs(YTest_actual - YTestPred_gru_actual));
    rmse_test_gru = sqrt(mean((YTest_actual - YTestPred_gru_actual).^2));
    mape_test_gru = mean(abs((YTest_actual - YTestPred_gru_actual)./YTest_actual)) * 100;
    corrMatrix_test_gru = corrcoef(YTest_actual, YTestPred_gru_actual);
    r_test_gru = corrMatrix_test_gru(1,2);
    fprintf('GRU模型测试集性能: MAE = %.4f, RMSE = %.4f, MAPE = %.2f%%, R = %.4f\n', mae_test_gru, rmse_test_gru, mape_test_gru, r_test_gru);
catch e
    warning('GRU模型训练失败：%s', e.message);
    mae_test_gru = NaN; rmse_test_gru = NaN; mape_test_gru = NaN; r_test_gru = NaN;
    YTestPred_gru_actual = NaN * ones(size(YTest_actual));
end

% 基准模型3: 简单CNN模型
layers_cnn = [
    sequenceInputLayer(numFeatures, 'MinLength', minSeqLength)
    convolution1dLayer(filterSize1, numFilters1, 'Padding', 'same')
    reluLayer
    maxPooling1dLayer(2, 'Stride', 2, 'Padding', 'same')
    convolution1dLayer(filterSize1, numFilters1*2, 'Padding', 'same')
    reluLayer
    globalAveragePooling1dLayer
    fullyConnectedLayer(1)
    regressionLayer];

options_cnn = trainingOptions('adam', ...
    'MaxEpochs', 80, ...
    'MiniBatchSize', batchSize, ...
    'InitialLearnRate', learnRate, ...
    'L2Regularization', l2reg, ...
    'GradientThreshold', 1, ...
    'ValidationData', {XVal, YVal}, ...
    'ValidationFrequency', 10, ...
    'ValidationPatience', 5, ...
    'Plots', 'none', ...
    'Verbose', true);

try
    net_cnn = trainNetwork(XTrain, YTrain, layers_cnn, options_cnn);
    YTestPred_cnn = predict(net_cnn, XTest);
    YTestPred_cnn_actual = YTestPred_cnn * (maxSpeed - minSpeed) + minSpeed;
    mae_test_cnn = mean(abs(YTest_actual - YTestPred_cnn_actual));
    rmse_test_cnn = sqrt(mean((YTest_actual - YTestPred_cnn_actual).^2));
    mape_test_cnn = mean(abs((YTest_actual - YTestPred_cnn_actual)./YTest_actual)) * 100;
    corrMatrix_test_cnn = corrcoef(YTest_actual, YTestPred_cnn_actual);
    r_test_cnn = corrMatrix_test_cnn(1,2);
    fprintf('CNN模型测试集性能: MAE = %.4f, RMSE = %.4f, MAPE = %.2f%%, R = %.4f\n', mae_test_cnn, rmse_test_cnn, mape_test_cnn, r_test_cnn);
catch e
    warning('CNN模型训练失败：%s', e.message);
    mae_test_cnn = NaN; rmse_test_cnn = NaN; mape_test_cnn = NaN; r_test_cnn = NaN;
    YTestPred_cnn_actual = NaN * ones(size(YTest_actual));
end

% 基准模型4: 修正的Transformer模型
% 使用简化的基于CNN和注意力的架构来模拟Transformer
embeddingDim = 64;
layers_transformer = [
    sequenceInputLayer(numFeatures, 'MinLength', minSeqLength, 'Name', 'input')
    
    % 特征投影到嵌入维度
    fullyConnectedLayer(embeddingDim, 'Name', 'embedding_projection')
    
    % 第一个卷积块 - 模拟多头注意力机制
    convolution1dLayer(3, embeddingDim, 'Padding', 'same', 'Name', 'conv_attn_1')
    reluLayer('Name', 'relu_attn_1')
    layerNormalizationLayer('Name', 'ln_1')
    dropoutLayer(0.1, 'Name', 'dropout_1')
    
    % 第二个卷积块 - 模拟前馈网络
    convolution1dLayer(1, embeddingDim*2, 'Padding', 'same', 'Name', 'conv_ffn_1')
    reluLayer('Name', 'relu_ffn_1')
    convolution1dLayer(1, embeddingDim, 'Padding', 'same', 'Name', 'conv_ffn_2')
    layerNormalizationLayer('Name', 'ln_2')
    dropoutLayer(0.1, 'Name', 'dropout_2')
    
    % 第三个注意力块
    convolution1dLayer(5, embeddingDim, 'Padding', 'same', 'Name', 'conv_attn_2')
    reluLayer('Name', 'relu_attn_2')
    layerNormalizationLayer('Name', 'ln_3')
    dropoutLayer(0.1, 'Name', 'dropout_3')
    
    % 全局平均池化
    globalAveragePooling1dLayer('Name', 'global_avg_pool')
    
    % 输出层
    fullyConnectedLayer(32, 'Name', 'fc_1')
    reluLayer('Name', 'relu_final')
    dropoutLayer(0.2, 'Name', 'dropout_final')
    fullyConnectedLayer(1, 'Name', 'output_fc')
    regressionLayer('Name', 'output')
];

% Transformer训练选项
options_transformer = trainingOptions('adam', ...
    'MaxEpochs', 100, ...
    'MiniBatchSize', min(32, batchSize), ...
    'InitialLearnRate', 5e-4, ...
    'LearnRateSchedule', 'piecewise', ...
    'LearnRateDropFactor', 0.8, ...
    'LearnRateDropPeriod', 20, ...
    'L2Regularization', 1e-4, ...
    'GradientThreshold', 1, ...
    'Shuffle', 'every-epoch', ...
    'ValidationData', {XVal, YVal}, ...
    'ValidationFrequency', 30, ...
    'ValidationPatience', 15, ...
    'Plots', 'none', ...
    'Verbose', true);

try
    % 训练Transformer模型
    fprintf('开始训练Transformer模型...\n');
    net_transformer = trainNetwork(XTrain, YTrain, layers_transformer, options_transformer);
    
    % 预测
    YTestPred_transformer = predict(net_transformer, XTest);
    YTestPred_transformer_actual = YTestPred_transformer * (maxSpeed - minSpeed) + minSpeed;
    
    % 评估
    mae_test_transformer = mean(abs(YTest_actual - YTestPred_transformer_actual));
    rmse_test_transformer = sqrt(mean((YTest_actual - YTestPred_transformer_actual).^2));
    mape_test_transformer = mean(abs((YTest_actual - YTestPred_transformer_actual)./YTest_actual)) * 100;
    corrMatrix_test_transformer = corrcoef(YTest_actual, YTestPred_transformer_actual);
    r_test_transformer = corrMatrix_test_transformer(1,2);
    
    fprintf('Transformer模型测试集性能: MAE = %.4f, RMSE = %.4f, MAPE = %.2f%%, R = %.4f\n', ...
        mae_test_transformer, rmse_test_transformer, mape_test_transformer, r_test_transformer);
catch e
    warning('Transformer模型训练失败：%s', e.message);
    mae_test_transformer = NaN; rmse_test_transformer = NaN; mape_test_transformer = NaN; r_test_transformer = NaN;
    YTestPred_transformer_actual = NaN * ones(size(YTest_actual));
end

%% ===== 最终模型的预测可视化（包含基准模型对比） =====
figure('Name','Final Model - Comprehensive Prediction Analysis with Benchmarks','Position',[100,100,1400,1000]);
    
% 子图1: 验证集预测效果 (优化模型)
subplot(2,3,1);
plot(YVal_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '验证集真实值'); hold on;
plot(YValPred_opt_actual, 'r--', 'LineWidth', 1.5, 'DisplayName', '优化模型预测');
xlabel('时间步'); ylabel('风速 (m/s)');
title(sprintf('验证集预测效果 (RMSE=%.3f)', rmse_val_opt));
legend('Location','best'); grid on;

% 子图2: 测试集预测效果 (优化模型)
subplot(2,3,2);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '测试集真实值'); hold on;
plot(YTestPred_opt_actual, 'r--', 'LineWidth', 1.5, 'DisplayName', '优化模型预测');
xlabel('时间步'); ylabel('风速 (m/s)');
title(sprintf('测试集预测效果 (RMSE=%.3f)', rmse_test_opt));
legend('Location','best'); grid on;

% 子图3: 测试集残差分析 (优化模型)
subplot(2,3,3);
errors_test_opt = YTest_actual - YTestPred_opt_actual;
histogram(errors_test_opt, 30, 'Normalization', 'probability');
xlabel('预测误差 (m/s)'); ylabel('概率密度'); 
title(sprintf('测试集误差分布 (均值=%.3f)', mean(errors_test_opt)));
grid on;

% 子图4: 测试集散点图 (优化模型)
subplot(2,3,4);
scatter(YTest_actual, YTestPred_opt_actual, 50, 'filled', 'MarkerFaceAlpha', 0.6);
hold on;
xy = [min(YTest_actual) max(YTest_actual)];
plot(xy, xy, 'k--', 'LineWidth', 2);
xlabel('真实值 (m/s)'); ylabel('预测值 (m/s)'); 
title(sprintf('预测 vs 真实 (R=%.3f)', r_test_opt));
grid on;
axis equal; axis tight;

% 子图5: 验证集和测试集性能对比 (优化模型 + 基准)
subplot(2,3,5);
metrics = {'MAE', 'RMSE', 'MAPE', 'R'};
opt_metrics = [mae_test_opt, rmse_test_opt, mape_test_opt, r_test_opt];
lstm_metrics = [mae_test_lstm, rmse_test_lstm, mape_test_lstm, r_test_lstm];
gru_metrics = [mae_test_gru, rmse_test_gru, mape_test_gru, r_test_gru];
cnn_metrics = [mae_test_cnn, rmse_test_cnn, mape_test_cnn, r_test_cnn];
transformer_metrics = [mae_test_transformer, rmse_test_transformer, mape_test_transformer, r_test_transformer];

x = categorical(metrics);
bar(x, [opt_metrics; lstm_metrics; gru_metrics; cnn_metrics; transformer_metrics]');
legend({'优化CNN-BiLSTM', 'LSTM', 'GRU', 'CNN', 'Transformer'}, 'Location', 'best');
ylabel('指标值');
title('不同模型测试集性能对比');
grid on;

% 子图6: 预测误差时间序列 (优化模型)
subplot(2,3,6);
plot(abs(errors_test_opt), 'g-', 'LineWidth', 1);
xlabel('时间步'); ylabel('绝对误差 (m/s)');
title('优化模型测试集绝对误差时间序列');
grid on;
    
% 添加误差统计信息
mean_abs_error = mean(abs(errors_test_opt));
std_abs_error = std(abs(errors_test_opt));
yline(mean_abs_error, 'r--', sprintf('均值=%.3f', mean_abs_error), 'LineWidth', 2);
yline(mean_abs_error + std_abs_error, 'r:', sprintf('+1σ=%.3f', mean_abs_error + std_abs_error));
yline(mean_abs_error - std_abs_error, 'r:', sprintf('-1σ=%.3f', max(0, mean_abs_error - std_abs_error)));

%% ===== 新增可视化：不同模型的真实值和预测值对比 =====
figure('Name', 'Models Prediction vs Actual Comparison', 'Position', [100, 100, 1400, 800]);

subplot(2,3,1);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '真实值'); hold on;
plot(YTestPred_opt_actual, 'r--', 'LineWidth', 1.5, 'DisplayName', '优化模型');
title('优化模型预测 vs 真实');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

subplot(2,3,2);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '真实值'); hold on;
plot(YTestPred_lstm_actual, 'g--', 'LineWidth', 1.5, 'DisplayName', 'LSTM');
title('LSTM预测 vs 真实');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

subplot(2,3,3);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '真实值'); hold on;
plot(YTestPred_gru_actual, 'm--', 'LineWidth', 1.5, 'DisplayName', 'GRU');
title('GRU预测 vs 真实');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

subplot(2,3,4);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '真实值'); hold on;
plot(YTestPred_cnn_actual, 'c--', 'LineWidth', 1.5, 'DisplayName', 'CNN');
title('CNN预测 vs 真实');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

subplot(2,3,5);
plot(YTest_actual, 'b-', 'LineWidth', 1.5, 'DisplayName', '真实值'); hold on;
plot(YTestPred_transformer_actual, 'y--', 'LineWidth', 1.5, 'DisplayName', 'Transformer');
title('Transformer预测 vs 真实');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

% 子图6: 所有模型对比
subplot(2,3,6);
plot(YTest_actual, 'k-', 'LineWidth', 2, 'DisplayName', '真实值'); hold on;
plot(YTestPred_opt_actual, 'r--', 'LineWidth', 1, 'DisplayName', '优化模型');
plot(YTestPred_lstm_actual, 'g--', 'LineWidth', 1, 'DisplayName', 'LSTM');
plot(YTestPred_gru_actual, 'm--', 'LineWidth', 1, 'DisplayName', 'GRU');
plot(YTestPred_cnn_actual, 'c--', 'LineWidth', 1, 'DisplayName', 'CNN');
plot(YTestPred_transformer_actual, 'y--', 'LineWidth', 1, 'DisplayName', 'Transformer');
title('所有模型预测对比');
xlabel('时间步'); ylabel('风速 (m/s)');
legend('Location', 'best'); grid on;

%% ===== 新增可视化：不同模型的预测误差情况 =====
figure('Name', 'Models Prediction Error Comparison', 'Position', [200, 200, 1400, 800]);

errors_opt = YTest_actual - YTestPred_opt_actual;
errors_lstm = YTest_actual - YTestPred_lstm_actual;
errors_gru = YTest_actual - YTestPred_gru_actual;
errors_cnn = YTest_actual - YTestPred_cnn_actual;
errors_transformer = YTest_actual - YTestPred_transformer_actual;

subplot(2,3,1);
histogram(errors_opt, 30, 'Normalization', 'probability', 'FaceColor', 'r', 'FaceAlpha', 0.7);
title('优化模型误差分布'); xlabel('误差 (m/s)'); ylabel('概率');
grid on;

subplot(2,3,2);
histogram(errors_lstm, 30, 'Normalization', 'probability', 'FaceColor', 'g', 'FaceAlpha', 0.7);
title('LSTM误差分布'); xlabel('误差 (m/s)'); ylabel('概率');
grid on;

subplot(2,3,3);
histogram(errors_gru, 30, 'Normalization', 'probability', 'FaceColor', 'm', 'FaceAlpha', 0.7);
title('GRU误差分布'); xlabel('误差 (m/s)'); ylabel('概率');
grid on;

subplot(2,3,4);
histogram(errors_cnn, 30, 'Normalization', 'probability', 'FaceColor', 'c', 'FaceAlpha', 0.7);
title('CNN误差分布'); xlabel('误差 (m/s)'); ylabel('概率');
grid on;

subplot(2,3,5);
histogram(errors_transformer, 30, 'Normalization', 'probability', 'FaceColor', 'y', 'FaceAlpha', 0.7);
title('Transformer误差分布'); xlabel('误差 (m/s)'); ylabel('概率');
grid on;

% 子图6: 误差统计对比
subplot(2,3,6);
model_names = {'优化模型', 'LSTM', 'GRU', 'CNN', 'Transformer'};
error_means = [mean(abs(errors_opt)), mean(abs(errors_lstm)), mean(abs(errors_gru)), ...
               mean(abs(errors_cnn)), mean(abs(errors_transformer))];
error_stds = [std(abs(errors_opt)), std(abs(errors_lstm)), std(abs(errors_gru)), ...
              std(abs(errors_cnn)), std(abs(errors_transformer))];

x = categorical(model_names);
bar(x, error_means);
hold on;
errorbar(x, error_means, error_stds, 'k.', 'LineWidth', 1.5);
title('各模型绝对误差统计');
ylabel('绝对误差 (m/s)');
grid on;
hold off;

%% 6. 生成综合报告可视化
figure('Name', 'Comprehensive Optimization Report', 'Position', [400, 400, 1600, 1200]);

% 子图1: 优化历程总览
subplot(3,4,1);
plot(1:maxGenerations, bestPerformanceHistory, 'bo-', 'LineWidth', 2);
xlabel('代数'); ylabel('最优 RMSE (m/s)');
title('优化历程 - 最优RMSE');
grid on;

subplot(3,4,2);
plot(1:maxGenerations, bestComplexityHistory, 'ro-', 'LineWidth', 2);
xlabel('代数'); ylabel('对应复杂度');
title('优化历程 - 对应复杂度');
grid on;

% 子图3: Pareto解数量变化
subplot(3,4,3);
numPareto = zeros(maxGenerations,1);
for g = 1:maxGenerations
    paretoStruct = allParetoFronts{g};
    numPareto(g) = paretoStruct.numSolutions;
end
bar(1:maxGenerations, numPareto);
xlabel('代数'); ylabel('Pareto解数量');
title('各代Pareto解数量');
grid on;

% 子图4: 超参数分布（最终代）
subplot(3,4,4);
finalParetoStruct = allParetoFronts{end};
if finalParetoStruct.numSolutions > 0
    optimizerDist = finalParetoStruct.params(:, 11);
    histogram(optimizerDist, 'BinEdges', [0.5,1.5,2.5,3.5]);
    xticks([1 2 3]);
    xticklabels({'Adam','SGDM','RMSprop'});
    xlabel('优化器类型'); ylabel('数量');
    title('最终Pareto解优化器分布');
    grid on;
end

% 子图5-8: 关键超参数进化
keyParams = [1, 2, 6, 7]; % BatchSize, LearnRate, LSTMUnits1, LSTMUnits2
paramTitles = {'BatchSize进化', 'LearnRate进化', 'LSTM Units1进化', 'LSTM Units2进化'};

for i = 1:4
    subplot(3,4,4+i);
    paramIdx = keyParams(i);
    paramEvolution = bestParamsHistory(:, paramIdx);
    plot(1:maxGenerations, paramEvolution, 'o-', 'LineWidth', 1.5);
    xlabel('代数'); ylabel(paramTitles{i});
    title(paramTitles{i});
    grid on;
end

% 子图9: 最终Pareto前沿详细分析
subplot(3,4,9);
finalParetoStruct = allParetoFronts{end};
if finalParetoStruct.numSolutions > 0
    perf_vals = finalParetoStruct.performance;
    complexity_vals = finalParetoStruct.complexity;
    scatter(complexity_vals, perf_vals, 100, 'filled');
    xlabel('模型复杂度'); ylabel('预测误差 RMSE (m/s)');
    title(sprintf('最终Pareto前沿 (%d个解)', finalParetoStruct.numSolutions));
    grid on;
    
    % 标注最优解（最小RMSE）
    [min_rmse, min_idx] = min(perf_vals);
    hold on;
    plot(complexity_vals(min_idx), min_rmse, 'r*', 'MarkerSize', 15, 'LineWidth', 3);
    text(complexity_vals(min_idx), min_rmse + 0.01, '最优解', 'HorizontalAlignment', 'center');
    
    % 添加：标注折中最优解
    % 计算归一化值（与前面选择折中解时一致）
    normalized_perf = (perf_vals - min(perf_vals)) / (max(perf_vals) - min(perf_vals) + eps);
    normalized_comp = (complexity_vals - min(complexity_vals)) / (max(complexity_vals) - min(complexity_vals) + eps);
    trade_off_scores = sqrt(normalized_perf.^2 + normalized_comp.^2);
    [~, tradeOffIdx] = min(trade_off_scores);
    
    % 标记折中最优解
    plot(complexity_vals(tradeOffIdx), perf_vals(tradeOffIdx), 'go', 'MarkerSize', 12, 'LineWidth', 3, 'MarkerFaceColor', 'g');
    text(complexity_vals(tradeOffIdx), perf_vals(tradeOffIdx) - 0.02, '折中最优解', 'HorizontalAlignment', 'center');
    
    hold off;
end
% 子图10: 性能改进趋势
subplot(3,4,10);
if length(bestPerformanceHistory) > 1
    improvement = (bestPerformanceHistory(1) - bestPerformanceHistory) ./ bestPerformanceHistory(1) * 100;
    plot(1:maxGenerations, improvement, 'g-', 'LineWidth', 2);
    xlabel('代数'); ylabel('改进百分比 (%)');
    title('相对初始代的性能改进');
    grid on;
end

% 子图11: 复杂度vs性能权衡分析
subplot(3,4,11);
finalParetoStruct = allParetoFronts{end};
if finalParetoStruct.numSolutions > 0
    perf_vals = finalParetoStruct.performance;
    complexity_vals = finalParetoStruct.complexity;
    
    % 计算权衡比率
    normalized_perf = (perf_vals - min(perf_vals)) / (max(perf_vals) - min(perf_vals));
    normalized_comp = (complexity_vals - min(complexity_vals)) / (max(complexity_vals) - min(complexity_vals));
    tradeoff_ratio = normalized_perf ./ (normalized_comp + eps);
    
    [sorted_ratio, sort_idx] = sort(tradeoff_ratio);
    plot(1:length(sorted_ratio), sorted_ratio, 'bo-', 'LineWidth', 1.5);
    xlabel('解的排序'); ylabel('权衡比率');
    title('性能-复杂度权衡分析');
    grid on;
end

% 子图12: 模型性能对比雷达图（简化版）
subplot(3,4,12);
% 创建性能对比表格
model_performance = [
    mae_test_opt, rmse_test_opt, mape_test_opt, r_test_opt;
    mae_test_lstm, rmse_test_lstm, mape_test_lstm, r_test_lstm;
    mae_test_gru, rmse_test_gru, mape_test_gru, r_test_gru;
    mae_test_cnn, rmse_test_cnn, mape_test_cnn, r_test_cnn;
    mae_test_transformer, rmse_test_transformer, mape_test_transformer, r_test_transformer
];

% 归一化性能指标
norm_perf = model_performance;
% 对于MAE, RMSE, MAPE，使用倒数进行归一化
for i = 1:3
    max_val = max(model_performance(:,i));
    min_val = min(model_performance(:,i));
    norm_perf(:,i) = (max_val - model_performance(:,i)) / (max_val - min_val);
end
% 对于R，直接归一化
max_r = max(model_performance(:,4));
min_r = min(model_performance(:,4));
norm_perf(:,4) = (model_performance(:,4) - min_r) / (max_r - min_r);

% 绘制堆叠条形图
x_models = categorical({'优化模型', 'LSTM', 'GRU', 'CNN', 'Transformer'});
b = bar(x_models, norm_perf, 'stacked');
legend({'MAE', 'RMSE', 'MAPE', 'R'}, 'Location', 'best');
title('归一化性能综合对比');
ylabel('归一化分数');
grid on;

% 添加：突出显示优化模型（折中最优解）
hold on;
xline(1, 'r--', '折中最优解', 'LineWidth', 2);
hold off;
%% 可视化指定代数的帕累托前沿
figure('Name', 'Pareto Fronts for Specific Generations', 'Position', [100, 100, 1200, 800]);

% 指定要可视化的代数
generationsToShow = [1, 5, 10, 20];

for i = 1:length(generationsToShow)
    gen = generationsToShow(i);
    subplot(2, 2, i); % 创建 2x2 的子图布局
    
    % 检查是否记录了该代的数据
    if gen <= maxGenerations
        paretoStruct = allParetoFronts{gen};
        if paretoStruct.numSolutions > 0
            % 获取性能和复杂度值
            perf_vals = paretoStruct.performance;
            complexity_vals = paretoStruct.complexity;
            
            % 绘制帕累托前沿
            scatter(complexity_vals, perf_vals, 50, 'filled');
            xlabel('模型复杂度');
            ylabel('预测误差 RMSE (m/s)');
            title(sprintf('第 %d 代帕累托前沿', gen));
            grid on;
        else
            text(0.5, 0.5, sprintf('第 %d 代无有效帕累托解', gen), ...
                 'HorizontalAlignment', 'center', 'FontSize', 12);
        end
    else
        text(0.5, 0.5, sprintf('第 %d 代数据未记录', gen), ...
             'HorizontalAlignment', 'center', 'FontSize', 12);
    end
end

%% 可视化指定代数的帕累托前沿
figure('Name', 'Pareto Fronts for Specific Generations', 'Position', [100, 100, 1200, 800]);

% 定义颜色（每种颜色对应一个代数）
colors = {'r', 'g', 'b', 'm', 'c', 'y'};

% 指定要可视化的代数
generationsToShow = [1, 5, 10, 20];

for i = 1:length(generationsToShow)
    gen = generationsToShow(i);
    subplot(2, 2, i); % 创建 2x2 的子图布局
    
    % 检查是否记录了该代的数据
    if gen <= maxGenerations
        paretoStruct = allParetoFronts{gen};
        if paretoStruct.numSolutions > 0
            % 获取性能和复杂度值
            perf_vals = paretoStruct.performance;
            complexity_vals = paretoStruct.complexity;
            
            % 绘制帕累托前沿
            scatter(complexity_vals, perf_vals, 60, colors{mod(i-1, length(colors)) + 1}, 'filled');
            xlabel('模型复杂度');
            ylabel('预测误差 RMSE (m/s)');
            title(sprintf('第 %d 代帕累托前沿', gen));
            grid on;
        else
            text(0.5, 0.5, sprintf('第 %d 代无有效帕累托解', gen), ...
                 'HorizontalAlignment', 'center', 'FontSize', 12);
        end
    else
        text(0.5, 0.5, sprintf('第 %d 代数据未记录', gen), ...
             'HorizontalAlignment', 'center', 'FontSize', 12);
    end
end



%% 可视化指定代数的帕累托前沿到同一张图（无连线，仅散点）
figure('Name', 'Pareto Fronts for Generations 1, 5, 10, 20', 'Position', [100, 100, 800, 600]);

% 1. 定义颜色+标记（去掉'-'线型，避免连线）
% 格式：[颜色][标记]，无任何线型符号
colors = {'ro', 'gx', 'b^', 'mv'};  % 分别对应：第1代(红圈)、第5代(绿叉)、第10代(蓝三角)、第20代(紫倒三角)
generationLabels = {'第1代', '第5代', '第10代', '第20代'};
% 定义点大小（可选，让散点更清晰）
markerSizes = [6, 6, 6, 6];

hold on;
for i = 1:length(generationsToShow)
    gen = generationsToShow(i);
    % 检查代数是否在有效范围内
    if gen <= maxGenerations
        paretoStruct = allParetoFronts{gen};
        % 仅当该代有有效帕累托解时绘制
        if paretoStruct.numSolutions > 0
            % 获取性能（RMSE）和复杂度数据
            perf_vals = paretoStruct.performance;
            complexity_vals = paretoStruct.complexity;
            
            % 2. 用scatter绘制散点（无连线），替代原plot的连线逻辑
            scatter(complexity_vals, perf_vals, ...
                    markerSizes(i), ...  % 点大小
                    colors{i}(1), ...     % 点颜色（提取colors中的颜色字符，如'r'）
                    colors{i}(2), ...     % 点标记（提取colors中的标记字符，如'o'）
                    'filled', ...         % 点填充（可选，让散点更醒目）
                    'MarkerEdgeColor', 'k', ...  % 点边缘颜色（可选，增强区分度）
                    'DisplayName', generationLabels{i});
        end
    else
        % 若指定代数超过实际最大代数，提示跳过
        fprintf('警告：指定代数%d超过实际最大代数%d，已跳过该代绘制\n', gen, maxGenerations);
    end
end
hold off;

% 图表标注与美化
xlabel('模型复杂度（越小越好）', 'FontSize', 11);
ylabel('预测误差 RMSE (m/s)（越小越好）', 'FontSize', 11);
title('第1、5、10、20代帕累托前沿对比（无连线散点图）', 'FontSize', 12);
grid on;  % 显示网格
legend('show', 'Location', 'best', 'FontSize', 10);  % 显示图例并放在最优位置
box on;  % 显示坐标轴边框（增强图表完整性）

% 保存优化结果（保持原逻辑不变）
finalParetoStruct = allParetoFronts{end};
save('nsga2_optimization_results_extended.mat', 'finalParetoStruct', 'allParetoFronts', 'bestParamsHistory', ...
     'bestPerformanceHistory', 'bestComplexityHistory', 'specificGenData');
fprintf('优化结果已保存至 nsga2_optimization_results_extended.mat\n');

%% 7. 打印优化后的超参数（扩展后的参数列表）
paramNames = {'Batch Size', 'Learn Rate', 'Pool Type', ...
              'Num Filters1', 'Filter Size1', 'LSTM Units1', ...
              'LSTM Units2', 'Reg Type', 'Dropout Prob1', 'Dropout Prob2', ...
              'OptimizerType', 'NumConvLayers', 'ConvDropout', 'ActivationFunction'};

fprintf('\n=== 最终优化的超参数 ===\n');
for i = 1:length(bestParams)
    if ismember(i, intCon)
        fprintf('%s: %d\n', paramNames{i}, round(bestParams(i)));
    else
        fprintf('%s: %.6f\n', paramNames{i}, bestParams(i));
    end
end

%% 8. 生成优化摘要报告
fprintf('\n=== NSGA-II 优化摘要报告 ===\n');
fprintf('总进化代数: %d\n', maxGenerations);
fprintf('种群大小: %d\n', populationSize);
fprintf('最终Pareto解数量: %d\n', finalParetoStruct.numSolutions);
fprintf('初始最优RMSE: %.4f (m/s)\n', bestPerformanceHistory(1));
fprintf('最终最优RMSE: %.4f (m/s)\n', bestPerformanceHistory(end));
fprintf('性能改进: %.2f%%\n', (bestPerformanceHistory(1) - bestPerformanceHistory(end)) / bestPerformanceHistory(1) * 100);

if exist('rmse_test_opt', 'var')
    fprintf('\n=== 最终模型测试集性能 ===\n');
    fprintf('优化模型: MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%, R=%.4f\n', mae_test_opt, rmse_test_opt, mape_test_opt, r_test_opt);
end

fprintf('\n=== 基准模型测试集性能对比 ===\n');
fprintf('LSTM模型: MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%, R=%.4f\n', mae_test_lstm, rmse_test_lstm, mape_test_lstm, r_test_lstm);
fprintf('GRU模型: MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%, R=%.4f\n', mae_test_gru, rmse_test_gru, mape_test_gru, r_test_gru);
fprintf('CNN模型: MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%, R=%.4f\n', mae_test_cnn, rmse_test_cnn, mape_test_cnn, r_test_cnn);
fprintf('Transformer模型: MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%, R=%.4f\n', mae_test_transformer, rmse_test_transformer, mape_test_transformer, r_test_transformer);

fprintf('\n可视化图表已生成，包括:\n');
fprintf('1. 最终模型预测分析（包含基准对比）\n');
fprintf('2. 模型预测 vs 真实对比\n');
fprintf('3. 模型预测误差分布对比\n');
fprintf('4. 综合优化报告\n');

fprintf('\n优化完成！所有结果已保存至 nsga2_optimization_results_extended.mat\n');



%% 7. 打印优化后的超参数（扩展后的参数列表）
paramNames = {'Batch Size', 'Learn Rate', 'Pool Type', ...
              'Num Filters1', 'Filter Size1', 'LSTM Units1', ...
              'LSTM Units2', 'Reg Type', 'Dropout Prob1', 'Dropout Prob2', ...
              'OptimizerType', 'NumConvLayers', 'ConvDropout', 'ActivationFunction'};

fprintf('\n=== 最终优化的超参数 ===\n');
for i = 1:length(bestParams)
    if ismember(i, intCon)
        fprintf('%s: %d\n', paramNames{i}, round(bestParams(i)));
    else
        fprintf('%s: %.6f\n', paramNames{i}, bestParams(i));
    end
end


%% ================== 局部辅助函数 ================== %% 

%% 初始化种群
function population = initializePopulation(populationSize, lb, ub, intCon)
    numVars = length(lb);
    population = lb + (ub - lb) .* rand(populationSize, numVars);
    for i = 1:populationSize
        for j = intCon
            population(i, j) = round(population(i, j));
        end
    end
end

%% 评估种群
function [performance, complexity, rmse, mae, mape, r] = evaluatePopulation(population, XTrain, YTrain, XVal, YVal, minSpeed, maxSpeed, sequenceLength, numFeatures)
    popSize = size(population, 1);
    performance = zeros(popSize, 1);
    complexity = zeros(popSize, 1);
    rmse = zeros(popSize, 1);
    mae = zeros(popSize, 1);
    mape = zeros(popSize, 1);
    r = zeros(popSize, 1);

    for i = 1:popSize
        [performance(i), complexity(i), rmse(i), mae(i), mape(i), r(i)] = evaluateModel(population(i, :), XTrain, YTrain, XVal, YVal, minSpeed, maxSpeed, sequenceLength, numFeatures);
    end
end

%% evaluateModel：解码超参数、建立网络、训练、评估
function [performance, complexity, rmse, mae, mape, r] = evaluateModel(x, XTrain, YTrain, XVal, YVal, minSpeed, maxSpeed, sequenceLength, numFeatures)
    performance = 1e6; complexity = 1e6; rmse = 1e6; mae = 1e6; mape = 1e6; r = -1;
    % 解码（同上顺序）
    batchSize = round(x(1));
    learnRate = x(2);
    poolType = round(x(3));
    numFilters1 = round(x(4));
    filterSize1 = round(x(5));
    lstmUnits1 = round(x(6));
    lstmUnits2 = round(x(7));
    regType = round(x(8));
    dropoutProb1 = x(9);
    dropoutProb2 = x(10);
    optimizerType = round(x(11));
    numConvLayers = round(x(12));
    convDropout = x(13);
    activationFunction = round(x(14)); % 激活函数类型

    % 参数检验
    if batchSize < 8 || batchSize > 2048 || ...
       learnRate < 1e-6 || learnRate > 5e-2 || ...
       ~ismember(poolType, [1,2]) || ...
       numFilters1 < 8 || numFilters1 > 512 || ...
       filterSize1 < 2 || filterSize1 > 5 || ...
       lstmUnits1 < 8 || lstmUnits1 > 512 || ...
       lstmUnits2 < 8 || lstmUnits2 > 512 || ...
       ~ismember(regType, [1,2,3]) || ...
       dropoutProb1 < 0 || dropoutProb1 > 0.6 || ...
       dropoutProb2 < 0 || dropoutProb2 > 0.6 || ...
       ~ismember(optimizerType, [1,2,3]) || ...
       ~ismember(numConvLayers, [1,2]) || ...
       convDropout < 0 || convDropout > 0.6 || ...
       ~ismember(activationFunction, [1, 2, 3, 4])
        return;
    end

    % 最短序列长度
    minSeqLength = min(cellfun(@length, XTrain));

    % 为 1D conv 设定合法 padding（避免 'valid' 字符串引起报错）
    convPadding = 'same'; % 选择 'same' 以保证兼容性

    % 构建网络
    layers = [
        sequenceInputLayer(numFeatures, 'MinLength', minSeqLength)
        convolution1dLayer(filterSize1, numFilters1, 'Padding', convPadding)
        batchNormalizationLayer
    ];
    
    % 添加激活函数层
    switch activationFunction
        case 1 % ReLU
            layers = [layers; reluLayer];
        case 2 % LeakyReLU
            layers = [layers; leakyReluLayer(0.01)];
        case 3 % Tanh
            layers = [layers; tanhLayer];
        case 4 % Sigmoid
            layers = [layers; sigmoidLayer];
    end

    if convDropout > 0
        layers = [layers; dropoutLayer(convDropout)];
    end

    if numConvLayers == 2
        numFilters2 = max(8, round(numFilters1 / 2));
        layers = [layers;
            convolution1dLayer(filterSize1, numFilters2, 'Padding', convPadding)
            batchNormalizationLayer];
        
        % 第二个卷积层的激活函数
        switch activationFunction
            case 1 % ReLU
                layers = [layers; reluLayer];
            case 2 % LeakyReLU
                layers = [layers; leakyReluLayer(0.01)];
            case 3 % Tanh
                layers = [layers; tanhLayer];
            case 4 % Sigmoid
                layers = [layers; sigmoidLayer];
        end
        
        if convDropout > 0
            layers = [layers; dropoutLayer(convDropout)];
        end
        rnnInputSize = numFilters2;
    else
        rnnInputSize = numFilters1;
    end

    % 池化（使用原来的 getPoolingLayer）
    layers = [layers; getPoolingLayer(poolType, sequenceLength, filterSize1)];

    % RNN：保留 BiLSTM 原始结构
    layers = [layers;
        bilstmLayer(lstmUnits1, 'OutputMode', 'sequence')
        dropoutLayer(dropoutProb1)
        bilstmLayer(lstmUnits2, 'OutputMode', 'last')
        dropoutLayer(dropoutProb2)
        fullyConnectedLayer(1)
        regressionLayer];

    % 正则化映射（近似）
    if regType == 1
        l2reg = 1e-4;
    elseif regType == 2
        l2reg = 0;
    elseif regType == 3
        l2reg = 1e-4;
    else
        l2reg = 0;
    end

    % 优化器映射
    if optimizerType == 1
        solver = 'adam';
    elseif optimizerType == 2
        solver = 'sgdm';
    else
        solver = 'rmsprop';
    end

    % trainingOptions
    try
        if strcmp(solver, 'sgdm')
            options = trainingOptions(solver, ...
                'MaxEpochs', 30, ...
                'MiniBatchSize', batchSize, ...
                'InitialLearnRate', learnRate, ...
                'L2Regularization', l2reg, ...
                'Momentum', 0.9, ...
                'GradientThreshold', 1, ...
                'ValidationData', {XVal, YVal}, ...
                'ValidationFrequency', 10, ...
                'ValidationPatience', 5, ...
                'Plots', 'none', ...
                'Verbose', false, ...
                'Shuffle', 'every-epoch');
        else
            options = trainingOptions(solver, ...
                'MaxEpochs', 30, ...
                'MiniBatchSize', batchSize, ...
                'InitialLearnRate', learnRate, ...
                'L2Regularization', l2reg, ...
                'GradientThreshold', 1, ...
                'ValidationData', {XVal, YVal}, ...
                'ValidationFrequency', 10, ...
                'ValidationPatience', 5, ...
                'Plots', 'none', ...
                'Verbose', false, ...
                'Shuffle', 'every-epoch');
        end

        % 训练并预测
        net = trainNetwork(XTrain, YTrain, layers, options);
        YPred = predict(net, XVal);

        % 反归一化
        YVal_actual = YVal * (maxSpeed - minSpeed) + minSpeed;
        YPred_actual = YPred * (maxSpeed - minSpeed) + minSpeed;

        % 性能指标
        mae = mean(abs(YVal_actual - YPred_actual));
        rmse = sqrt(mean((YVal_actual - YPred_actual).^2));
        mape = mean(abs((YVal_actual - YPred_actual)./YVal_actual)) * 100;
        corrMatrix = corrcoef(YVal_actual, YPred_actual);
        if ~isempty(corrMatrix) && size(corrMatrix,1) >= 2
            r = corrMatrix(1,2);
        else
            r = 0;
        end
        performance = rmse;

        % 复杂度计算：卷积 + RNN + 全连接
        complexity = 0;
        % conv1 parameters: filterSize * inChannels * outChannels + bias
        conv1_weights = filterSize1 * numFeatures * numFilters1;
        conv1_bias = numFilters1;
        complexity = complexity + conv1_weights + conv1_bias;
        if numConvLayers == 2
            numFilters2 = max(8, round(numFilters1 / 2));
            conv2_weights = filterSize1 * numFilters1 * numFilters2;
            conv2_bias = numFilters2;
            complexity = complexity + conv2_weights + conv2_bias;
            rnn_input = numFilters2;
        else
            rnn_input = numFilters1;
        end

        % BiLSTM 参数（保守估算）
        forwardParams1 = 4 * lstmUnits1 * (rnn_input + lstmUnits1) + 4 * lstmUnits1;
        backwardParams1 = forwardParams1;
        complexity = complexity + forwardParams1 + backwardParams1;
        inputSize2 = 2 * lstmUnits1;
        forwardParams2 = 4 * lstmUnits2 * (inputSize2 + lstmUnits2) + 4 * lstmUnits2;
        backwardParams2 = forwardParams2;
        complexity = complexity + forwardParams2 + backwardParams2;

        % 全连接层
        fcInput = 2 * lstmUnits2;
        fcWeights = fcInput * 1;
        fcBias = 1;
        complexity = complexity + fcWeights + fcBias;

        % RMSE 过高惩罚
        if rmse > 5
            performance = performance + 500;
            complexity = complexity + 500;
        end
    catch e
        warning('模型训练失败：%s', e.message);
        performance = 1e6; complexity = 1e6; rmse = 1e6; mae = 1e6; mape = 1e6; r = -1;
    end

    fprintf('性能指标: RMSE=%.4f (m/s), 复杂度=%d\n', rmse, complexity);
end

%% 快速非支配排序
function [fronts, rank] = fastNonDominatedSort(performance, complexity)
    popSize = length(performance);
    fronts = cell(1, popSize);
    rank = zeros(popSize, 1);
    dominationCount = zeros(popSize, 1);
    dominatedSolutions = cell(popSize, 1);
    for i = 1:popSize
        dominatedSolutions{i} = [];
    end
    for i = 1:popSize
        if performance(i) >= 1e6 || complexity(i) >= 1e6
            continue;
        end
        for j = 1:popSize
            if i ~= j && performance(j) < 1e6 && complexity(j) < 1e6
                if (performance(i) <= performance(j) && complexity(i) <= complexity(j)) && ...
                   (performance(i) < performance(j) || complexity(i) < complexity(j))
                    if ~ismember(j, dominatedSolutions{i})
                        dominatedSolutions{i} = [dominatedSolutions{i}, j];
                    end
                elseif (performance(j) <= performance(i) && complexity(j) <= complexity(i)) && ...
                       (performance(j) < performance(i) || complexity(j) < complexity(i))
                    dominationCount(i) = dominationCount(i) + 1;
                end
            end
        end
    end
    currentFront = 1;
    fronts{currentFront} = find(dominationCount == 0);
    while ~isempty(fronts{currentFront})
        nextFront = [];
        for i = 1:length(fronts{currentFront})
            idx = fronts{currentFront}(i);
            rank(idx) = currentFront;
            for j = 1:length(dominatedSolutions{idx})
                j_val = dominatedSolutions{idx}(j);
                dominationCount(j_val) = dominationCount(j_val) - 1;
                if dominationCount(j_val) == 0
                    nextFront = [nextFront, j_val];
                end
            end
        end
        currentFront = currentFront + 1;
        fronts{currentFront} = nextFront;
    end
    fronts = fronts(1:currentFront-1);
end

%% 改进的拥挤距离计算
function distance = improvedCrowdingDistance(performance, complexity, fronts)
    popSize = length(performance);
    distance = zeros(popSize, 1);
    for f = 1:length(fronts)
        if ~isempty(fronts{f})
            front = fronts{f};
            validFront = front(performance(front) < 1e6 & complexity(front) < 1e6);
            if isempty(validFront)
                continue;
            end
            epsillon = 1e-8;
            perfEps = performance(validFront) + epsillon * rand(size(performance(validFront)));
            compEps = complexity(validFront) + epsillon * rand(size(complexity(validFront)));
            [sortedPerf, idxPerf] = sort(perfEps);
            [sortedComp, idxComp] = sort(compEps);
            distance(validFront) = 0;
            if length(validFront) > 1
                distance(validFront(idxPerf(1))) = 1e6;
                distance(validFront(idxPerf(end))) = 1e6;
                distance(validFront(idxComp(1))) = 1e6;
                distance(validFront(idxComp(end))) = 1e6;
            end
            if length(validFront) > 2
                perfRange = max(sortedPerf) - min(sortedPerf);
                compRange = max(sortedComp) - min(sortedComp);
                if perfRange == 0
                    perfRange = eps;
                end
                if compRange == 0
                    compRange = eps;
                end
                for i = 2:length(validFront)-1
                    distance(validFront(idxPerf(i))) = distance(validFront(idxPerf(i))) + ...
                        (sortedPerf(i+1) - sortedPerf(i-1)) / perfRange;
                    distance(validFront(idxComp(i))) = distance(validFront(idxComp(i))) + ...
                        (sortedComp(i+1) - sortedComp(i-1)) / compRange;
                end
            end
        end
    end
end

%% 锦标赛选择
function matingPool = tournamentSelection(population, rank, distance, crossoverFraction, populationSize)
    popSize = size(population, 1);
    matingPoolSize = round(crossoverFraction * popSize);
    matingPool = zeros(matingPoolSize, size(population, 2));
    for i = 1:matingPoolSize
        idx1 = randi(populationSize);
        idx2 = randi(populationSize);
        if rank(idx1) < rank(idx2) || (rank(idx1) == rank(idx2) && distance(idx1) > distance(idx2))
            matingPool(i, :) = population(idx1, :);
        else
            matingPool(i, :) = population(idx2, :);
        end
    end
end

%% 交叉和变异
function offspring = crossoverAndMutation(matingPool, lb, ub, intCon, mutationRate)
    popSize = size(matingPool, 1);
    numVars = size(matingPool, 2);
    offspring = zeros(popSize, numVars);
    for i = 1:popSize
        for j = 1:numVars
            if ismember(j, intCon)
                matingPool(i, j) = round(matingPool(i, j));
            end
        end
    end
    for i = 1:2:popSize-1
        parent1 = matingPool(i, :);
        parent2 = matingPool(i+1, :);
        crossPoint = randi([1, numVars-1]);
        offspring(i, :) = [parent1(1:crossPoint), parent2(crossPoint+1:end)];
        offspring(i+1, :) = [parent2(1:crossPoint), parent1(crossPoint+1:end)];
    end
    for i = 1:popSize
        for j = 1:numVars
            if rand < mutationRate
                offspring(i, j) = lb(j) + (ub(j) - lb(j)) * rand;
                if ismember(j, intCon)
                    offspring(i, j) = round(offspring(i, j));
                end
            end
        end
    end
end

%% 环境选择
function [newPopulation, newPerformance, newComplexity] = environmentalSelection(...
    combinedPopulation, combinedPerformance, combinedComplexity, combinedRank, combinedDistance, populationSize)
    popSize = size(combinedPopulation, 1);
    newPopulation = zeros(populationSize, size(combinedPopulation, 2));
    newPerformance = zeros(populationSize, 1);
    newComplexity = zeros(populationSize, 1);
    fronts = unique(combinedRank);
    idx = 1;
    for i = 1:length(fronts)
        front = find(combinedRank == fronts(i));
        frontSize = length(front);
        if idx + frontSize <= populationSize
            newPopulation(idx:idx+frontSize-1, :) = combinedPopulation(front, :);
            newPerformance(idx:idx+frontSize-1) = combinedPerformance(front);
            newComplexity(idx:idx+frontSize-1) = combinedComplexity(front);
            idx = idx + frontSize;
        else
            [~, sortedIdx] = sort(combinedDistance(front) + 1e-6 * rand(size(combinedDistance(front))), 'descend');
            selected = front(sortedIdx(1:populationSize - idx + 1));
            newPopulation(idx:populationSize, :) = combinedPopulation(selected, :);
            newPerformance(idx:populationSize) = combinedPerformance(selected);
            newComplexity(idx:populationSize) = combinedComplexity(selected);
            break;
        end
    end
end

%% 查找最终 Pareto 前沿（修正为结构化输出）
function paretoStruct = findParetoFront(population, performance, complexity)
    % 输出结构化的Pareto前沿，包含超参数、性能目标、复杂度目标
    popSize = size(population, 1);
    isPareto = true(popSize, 1);  % 标记非支配解
    invalid = (performance >= 1e6) | (complexity >= 1e6);  % 标记无效解（训练失败）
    isPareto(invalid) = false;
    
    % 筛选有效解并检查支配关系
    validIndices = find(~invalid);
    for i = 1:length(validIndices)
        idx_i = validIndices(i);
        for j = 1:length(validIndices)
            if i == j, continue; end
            idx_j = validIndices(j);
            % 支配条件：j在所有目标上不劣于i，且至少一个目标更优（最小化问题）
            dominates = (performance(idx_j) <= performance(idx_i)) && ...
                        (complexity(idx_j) <= complexity(idx_i)) && ...
                        (performance(idx_j) < performance(idx_i) || complexity(idx_j) < complexity(idx_i));
            if dominates
                isPareto(idx_i) = false;
                break;
            end
        end
    end
    
    % 构造结构化输出
    paretoStruct = struct();
    if ~any(isPareto)
        % 无有效非支配解时，返回空字段
        paretoStruct.params = [];       % 超参数
        paretoStruct.performance = [];  % 性能目标（RMSE）
        paretoStruct.complexity = [];   % 复杂度目标
        paretoStruct.numSolutions = 0;  % 非支配解数量
        warning('未找到有效帕累托最优解，可能所有模型训练失败或性能极差。');
        return;
    end
    
    % 提取非支配解的超参数和目标值
    paretoStruct.params = population(isPareto, :);        % 超参数矩阵（每行一个解）
    paretoStruct.performance = performance(isPareto);     % 性能目标向量
    paretoStruct.complexity = complexity(isPareto);       % 复杂度目标向量
    paretoStruct.numSolutions = sum(isPareto);            % 非支配解数量
end

%% 获取池化层
function layer = getPoolingLayer(id, sequenceLength, filterSize)
    poolSize = min(3, floor(sequenceLength / 8));
    stride = min(2, poolSize);
    switch id
        case 1
            layer = maxPooling1dLayer(poolSize, 'Stride', stride, 'Padding', 'same', 'Name', 'maxpool');
        case 2
            layer = averagePooling1dLayer(poolSize, 'Stride', stride, 'Padding', 'same', 'Name', 'avgpool');
        otherwise
            layer = maxPooling1dLayer(poolSize, 'Stride', stride, 'Padding', 'same', 'Name', 'maxpool');
    end
end
